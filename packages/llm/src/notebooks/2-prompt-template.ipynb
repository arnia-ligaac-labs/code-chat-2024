{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring `Prompt Template` to customize input\n",
    "\n",
    "- General conversation with generic responses\n",
    "- Now we will have more control over convo and type of responses\n",
    "\n",
    "Let's create an application that makes a joke from any word we give as input.\n",
    "- No general conversation will be possible, only configured behaviour -> a joke AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Bedrock = require('@langchain/community/llms/bedrock').Bedrock;\n",
    "var ChatPromptTemplate = require('@langchain/core/prompts').ChatPromptTemplate;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the `model` client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = new Bedrock({\n",
    "    model_id:'amazon.titan-text-express-v1',\n",
    "    temperature: 1,\n",
    "    maxTokenCount: 512,\n",
    "    topP: 0.9,\n",
    "    verbose: true\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt template\n",
    "\n",
    "### `ChatPromptTemplate.fromTemplate`\n",
    "- sets up `system` message instructing LLM for behaviour using NL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var promptTemplate = ChatPromptTemplate.fromTemplate(\"You are a comendian. Give me a funny joke based on the following word {input}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence a prompt template takes input and parses it in a format that can be chained / consumed into the rest of the components. Advanced usages later on. Output of the formatter is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptTemplate.format({input: 'EXAMPLE'}).then((output) => console.log(output));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chain (sidenote)\n",
    "\n",
    "- `pipe`: Pass in waterfall style from one component to next the output as input\n",
    "\n",
    "### Basic\n",
    "\n",
    "```javascript\n",
    "const outputA = funcA();\n",
    "const outputB = funcB(outputA);\n",
    "```\n",
    "\n",
    "### Intermediate\n",
    "```javascript\n",
    "funcB(funcA())\n",
    "```\n",
    "\n",
    "### Advanced (piping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are a comendian. Give me a funny joke based on the following word EXAMPLE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5\n"
     ]
    }
   ],
   "source": [
    "class PipeableObject {\n",
    "    constructor(initialValue = null) {\n",
    "        this.value = initialValue;\n",
    "    }\n",
    "\n",
    "    pipe(func) {\n",
    "        this.value = func(this.value);\n",
    "        return this;\n",
    "    }\n",
    "\n",
    "    getValue() {\n",
    "        return this.value;\n",
    "    }\n",
    "}\n",
    "\n",
    "var obj = new PipeableObject(5);\n",
    "\n",
    "obj.pipe(value => value * 2) // Multiply by 2\n",
    "   .pipe(value => value + 3) // Add 3\n",
    "   .pipe(value => value / 2); // Divide by 2\n",
    "   \n",
    "console.log(obj.getValue());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chain (template)\n",
    "\n",
    "- We send the prompt to the model (pipe it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chainTemplate = promptTemplate.pipe(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM\n",
    "\n",
    "- Given input we invoke LLM and get a joke for the given input word using prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = 'dog';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chainTemplate.invoke({input}).then((output) => console.log(output));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt template (messages)\n",
    "\n",
    "### `ChatPromptTemplate.fromMessages`\n",
    "- explicitly set up `system` message instructing LLM for behaviour using `imperative` NL.\n",
    "- explicitly set up additional context and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"Human: You are a comendian. Give me a funny joke based on the following word dog\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "var promptMessages = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"You are a comendian. Generate a joke based on the word provided by the user.\"],\n",
    "    [\"user\", \"{input}\"]\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chain (messages)\n",
    "\n",
    "- We send the prompt to the model (pipe it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chainMessages = promptMessages.pipe(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM\n",
    "\n",
    "- Given input we invoke LLM and get a joke for the given input word using prompt messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"System: You are a comendian. Generate a joke based on the word provided by the user.\\nHuman: dog\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] [1.64s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nBot: What did the dog say after eating a bone?\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\n",
      "Bot: What did the dog say after eating a bone?\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] [1.86s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nQ: Why don't dogs have dentists?\\nA: Because they give them bones to chew on!\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\n",
      "Q: Why don't dogs have dentists?\n",
      "A: Because they give them bones to chew on!\n"
     ]
    }
   ],
   "source": [
    "chainMessages.invoke({input}).then((output) => console.log(output));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "20.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
