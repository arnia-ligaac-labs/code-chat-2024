{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LLM invocation\n",
    "\n",
    "We have a basic generic conversation with no context or history or bells & whistels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Bedrock = require('@langchain/community/llms/bedrock').Bedrock;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the `model` client\n",
    "\n",
    "1. Create a new Bedrock client instance.\n",
    "2. Pass `model_id`, name of model to use, see [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html)\n",
    "3. Pass `temperature`, how serious (`0`) / creative (`1`) the model should be.\n",
    "4. Pass `maxTokenCount`, how many tokens (cost) is to be spent by the LLM on generating a response.\n",
    "5. Pass `topP`, \n",
    "6. Pass `verbose`, to show detailed logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = new Bedrock({\n",
    "    model_id:'amazon.titan-text-express-v1',\n",
    "    temperature: 1,\n",
    "    maxTokenCount: 512,\n",
    "    topP: 0.9,\n",
    "    verbose: true\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure input for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"Write a haiku about a long journey\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM with input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"Write a haiku about a long journey\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] [3.02s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nIt took so long to get here\\nIt was worth the journey, though\\n\\nI made it, and I'm glad.\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "\n",
      "It took so long to get here\n",
      "It was worth the journey, though\n",
      "\n",
      "I made it, and I'm glad.\n"
     ]
    }
   ],
   "source": [
    "model.invoke(input).then((response) => console.log(response));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "20.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
