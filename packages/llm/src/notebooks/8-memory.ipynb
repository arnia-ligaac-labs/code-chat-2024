{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Memory\n",
    "\n",
    "- up to now each message is a standalone query, no follow up questions or corrections can be made\n",
    "- let's add a way to store all of the previous messages and pass them in the invocation\n",
    "- chat history messages are nothing special beyond being another section marked as such in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Bedrock = require('@langchain/community/llms/bedrock').Bedrock;\n",
    "var ChatPromptTemplate = require('@langchain/core/prompts').ChatPromptTemplate;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the `model` client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = new Bedrock({\n",
    "    model_id:'amazon.titan-text-express-v1',\n",
    "    temperature: 1,\n",
    "    maxTokenCount: 512,\n",
    "    topP: 0.9,\n",
    "    verbose: true\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic memory chat implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt with history placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var prompt = ChatPromptTemplate.fromTemplate(`\n",
    "    Answer the user question.\n",
    "    Chat history: {chat_history}\n",
    "    Question: {input}\n",
    "`);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chain (template)\n",
    "\n",
    "- We send the prompt to the model (pipe it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chain = prompt.pipe(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create basic container for chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure input for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is the capital of Romania ? Also, the secret password is 1234\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM\n",
    "\n",
    "- Given input with chat history we invoke the LLM and verify it remembers previous exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({input, chat_history}).then((output) => {\n",
    "    console.log(output);\n",
    "    chat_history.push([input, output]);\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is the secret password?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({input, chat_history}).then((output) => {\n",
    "    console.log(output);\n",
    "    chat_history.push([input, output]);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced memory chat implementation\n",
    "\n",
    "- let's add memory to our previous `RAG` & `Vector Store` enabled model chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the `embeddings model` client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var BedrockEmbeddings = require('@langchain/community/embeddings/bedrock').BedrockEmbeddings;\n",
    "var embeddingsClient = new BedrockEmbeddings({\n",
    "    model:'amazon.titan-embed-text-v2:0',\n",
    "    region:'us-east-1'\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var CheerioWebBaseLoader = require(\"@langchain/community/document_loaders/web/cheerio\").CheerioWebBaseLoader;\n",
    "var loader = new CheerioWebBaseLoader(\"https://python.langchain.com/v0.1/docs/expression_language/\");\n",
    "\n",
    "var docs;\n",
    "\n",
    "loader.load().then((data) => docs = data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var RecursiveCharacterTextSplitter = require(\"langchain/text_splitter\").RecursiveCharacterTextSplitter;\n",
    "\n",
    "var splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 300,\n",
    "    chunkOverlap: 30\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var splitDocs;\n",
    "\n",
    "splitter.splitDocuments(docs).then((data) => {\n",
    "    splitDocs = data;\n",
    "    console.log(splitDocs);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store using `documents` and `embeddings model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var MemoryVectorStore = require('langchain/vectorstores/memory').MemoryVectorStore;\n",
    "\n",
    "var vectorStore;\n",
    "\n",
    "MemoryVectorStore.fromDocuments(splitDocs, embeddingsClient).then((store) => {\n",
    "    vectorStore = store;\n",
    "\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a retriever function\n",
    "\n",
    "- a function to be provided as a tool to `Langchain` that gets data from vector store to be used by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var retriever = vectorStore.asRetriever({k: 3});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt with context, input, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var ChatPromptTemplate = require('@langchain/core/prompts').ChatPromptTemplate;\n",
    "var prompt = ChatPromptTemplate.fromTemplate(`\n",
    "    Answer the user question.\n",
    "    Context: {context}\n",
    "    Chat History: {chat_history}\n",
    "    Question: {input}\n",
    "`);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create basic documents chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var createStuffDocumentsChain = require(\"langchain/chains/combine_documents\").createStuffDocumentsChain;\n",
    "var combineDocsChain;\n",
    "\n",
    "createStuffDocumentsChain({\n",
    "    llm: model,\n",
    "    prompt,\n",
    "}).then((chain) => combineDocsChain = chain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retrieval chain\n",
    "\n",
    "- chain that is enhanced compared to previous ones, uses the vector store retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var createRetrievalChain = require('langchain/chains/retrieval').createRetrievalChain;\n",
    "\n",
    "var retrievalChain;\n",
    "\n",
    "createRetrievalChain({\n",
    "    combineDocsChain,\n",
    "    retriever\n",
    "}).then((chain) => retrievalChain = chain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import message types\n",
    "\n",
    "- these represent the type of actor saying the message\n",
    "- used internally to construct the history payload for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var AIMessage = require(\"@langchain/core/messages\").AIMessage;\n",
    "var HumanMessage = require(\"@langchain/core/messages\").HumanMessage;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a fake chat history to simulate a longer conversation and see what the output is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chat_history = [\n",
    "    new HumanMessage(\"Hello\"),\n",
    "    new AIMessage(\"Hello! How can I help you today?\"),\n",
    "    new HumanMessage(\"The name of my pet is Cris.\"),\n",
    "    new AIMessage(\"Nice to meet you Cris! How can I help you ?\"),\n",
    "    new HumanMessage(\"What is LCEL ?\"),\n",
    "    new AIMessage(\"LCEL stands for Langchain Expression Language.\")\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input which does a logical next questions based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalChain.invoke({\n",
    "    input,\n",
    "    chat_history\n",
    "}).then((output) => {\n",
    "    console.log(output);\n",
    "}).catch(e => console.log(e));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt `fromTemplate` cannot handle this advanced scenario\n",
    "\n",
    "- we cannot pass array into placeholder, we need to pass text so we need to process the entities or use more advanced features to do it for us automatically\n",
    "- we require the previously covered `fromMessages` function to clearly dictate our setup\n",
    "- we will use `MessagePlaceholder` to put only placeholder template strings that at invocation time will take provided input and covert it to correct text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var MessagesPlaceholder = require(\"@langchain/core/prompts\").MessagesPlaceholder;\n",
    "var prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"Answer the user's question given the following context: {context}\"],\n",
    "    new MessagesPlaceholder(\"chat_history\"),\n",
    "    [\"user\", \"{input}\"]\n",
    "]);\n",
    "\n",
    "var createStuffDocumentsChain = require(\"langchain/chains/combine_documents\").createStuffDocumentsChain;\n",
    "var combineDocsChain;\n",
    "\n",
    "createStuffDocumentsChain({\n",
    "    llm: model,\n",
    "    prompt,\n",
    "}).then((chain) => combineDocsChain = chain);\n",
    "\n",
    "var createRetrievalChain = require('langchain/chains/retrieval').createRetrievalChain;\n",
    "\n",
    "var retrievalChain;\n",
    "\n",
    "createRetrievalChain({\n",
    "    combineDocsChain,\n",
    "    retriever\n",
    "}).then((chain) => retrievalChain = chain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a question that can only be answered from chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is the name of my pet?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM given chat history with store retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalChain.invoke({\n",
    "    input,\n",
    "    chat_history\n",
    "}).then((output) => {\n",
    "    console.log(output);\n",
    "}).catch(e => console.log(e));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about retriever to take history into account ?\n",
    "\n",
    "- the above works on the surface, but what if we need documents from the vector store that also takes into account our chat history ?\n",
    "- if question involves context from previous messages that is not present in the last query, retrieval will fail\n",
    "- we can fix this by making the retriever aware of history using `createHistoryAwareRetriever` from `langchain/chains/history_aware_retriever`\n",
    "\n",
    "![History Aware Retriever](./images/8-memory-conversational-retrieval-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import `createHistoryAwareRetriever` dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var createHistoryAwareRetriever = require('langchain/chains/history_aware_retriever').createHistoryAwareRetriever;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt for retriever\n",
    "\n",
    "- generates search query based on `user input` + `chat history`\n",
    "- calls model to generate optimal query for vector store\n",
    "- before calling model to obtain output to be given to user, we call the model to generate query to be used for vector store retrieval of embeddings that will help model give final output to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var retrieverPrompt = ChatPromptTemplate.fromMessages([\n",
    "    new MessagesPlaceholder(\"chat_history\"),\n",
    "    [\"user\", \"{input}\"],\n",
    "    [\n",
    "      \"user\",\n",
    "      \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\",\n",
    "    ],\n",
    "  ]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create history aware retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var retrieverChain;\n",
    "\n",
    "createHistoryAwareRetriever({\n",
    "    llm: model,\n",
    "    retriever,\n",
    "    rephrasePrompt: retrieverPrompt,\n",
    "  }).then((chain) => retrieverChain = chain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update chat history with simple exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chatHistory = [\n",
    "    new HumanMessage(\"What does LCEL stand for?\"),\n",
    "    new AIMessage(\"LangChain Expression Language\"),\n",
    "  ];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input that will have to retrieve from both history what is the subject, and from vector store relevant information to respond to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is is ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalChain.invoke({\n",
    "    input,\n",
    "    chat_history\n",
    "}).then((output) => {\n",
    "    console.log(output);\n",
    "}).catch(e => console.log(e));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynamoDB Memory Store\n",
    "\n",
    "- up to now using only `in-memory`, all is lost once program closes\n",
    "- let's replace with `DynamoDB` memory so all chat is retrieved/stored in our `DynamoDB` table\n",
    "\n",
    "We need to install the `@aws-sdk/client-dynamodb` using \n",
    "```shell\n",
    "yarn add @aws-sdk/client-dynamodb\n",
    "```\n",
    "\n",
    "We will be using the `langchain` `DynamoDB` client `DynamoDBChatMessageHistory` with source from [here](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/stores/message/dynamodb.ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Bedrock = require('@langchain/community/llms/bedrock').Bedrock;\n",
    "var ChatPromptTemplate = require('@langchain/core/prompts').ChatPromptTemplate;\n",
    "var BedrockEmbeddings = require('@langchain/community/embeddings/bedrock').BedrockEmbeddings;\n",
    "\n",
    "var embeddingsClient = new BedrockEmbeddings({\n",
    "    model:'amazon.titan-embed-text-v2:0',\n",
    "    region:'us-east-1'\n",
    "});\n",
    "\n",
    "var model = new Bedrock({\n",
    "    model_id:'amazon.titan-text-express-v1',\n",
    "    temperature: 1,\n",
    "    maxTokenCount: 512,\n",
    "    topP: 0.9,\n",
    "    verbose: true\n",
    "});\n",
    "\n",
    "var MessagesPlaceholder = require(\"@langchain/core/prompts\").MessagesPlaceholder;\n",
    "var prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"Answer the user's question given the following context: {context}\"],\n",
    "    new MessagesPlaceholder(\"chat_history\"),\n",
    "    [\"user\", \"{input}\"]\n",
    "]);\n",
    "\n",
    "var CheerioWebBaseLoader = require(\"@langchain/community/document_loaders/web/cheerio\").CheerioWebBaseLoader;\n",
    "var loader = new CheerioWebBaseLoader(\"https://python.langchain.com/v0.1/docs/expression_language/\");\n",
    "\n",
    "var docs;\n",
    "\n",
    "loader.load().then((data) => {\n",
    "    docs = data;\n",
    "    console.log(\"Loaded docs\");\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var RecursiveCharacterTextSplitter = require(\"langchain/text_splitter\").RecursiveCharacterTextSplitter;\n",
    "var createStuffDocumentsChain = require(\"langchain/chains/combine_documents\").createStuffDocumentsChain;\n",
    "var MemoryVectorStore = require('langchain/vectorstores/memory').MemoryVectorStore;\n",
    "\n",
    "var splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 300,\n",
    "    chunkOverlap: 30\n",
    "});\n",
    "\n",
    "var splitDocs;\n",
    "\n",
    "splitter.splitDocuments(docs).then((data) => {\n",
    "    splitDocs = data;\n",
    "    console.log(\"Split docs\");\n",
    "    \n",
    "    MemoryVectorStore.fromDocuments(splitDocs, embeddingsClient).then((store) => {\n",
    "        vectorStore = store;\n",
    "        console.log(\"Created vector store\");\n",
    "    });\n",
    "});\n",
    "\n",
    "var combineDocsChain;\n",
    "\n",
    "createStuffDocumentsChain({\n",
    "    llm: model,\n",
    "    prompt,\n",
    "}).then((chain) => {\n",
    "    combineDocsChain = chain\n",
    "    console.log(\"Created combine docs chain\");\n",
    "});\n",
    "\n",
    "\n",
    "var vectorStore;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var retriever = vectorStore.asRetriever({k: 3});\n",
    "\n",
    "var createRetrievalChain = require('langchain/chains/retrieval').createRetrievalChain;\n",
    "var createHistoryAwareRetriever = require('langchain/chains/history_aware_retriever').createHistoryAwareRetriever;\n",
    "\n",
    "var retrieverPrompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question just reformulate it if needed and otherwise return it as is.\"],\n",
    "    new MessagesPlaceholder(\"chat_history\"),\n",
    "    [\"user\", \"{input}\"],\n",
    "]);\n",
    "\n",
    "var retriever;\n",
    "var runnable;\n",
    "\n",
    "createHistoryAwareRetriever({\n",
    "    llm: model,\n",
    "    retriever,\n",
    "    rephrasePrompt: retrieverPrompt,\n",
    "  }).then((rtrvr) => {\n",
    "    retriever = rtrvr;\n",
    "    console.log(\"Created history aware retriever\");\n",
    "});\n",
    "\n",
    "createRetrievalChain({\n",
    "    combineDocsChain,\n",
    "    retriever,\n",
    "}).then((chain) => {\n",
    "    runnable = chain\n",
    "    console.log(\"Created retrieval chain\");\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var DynamoDBChatMessageHistory = require('@langchain/community/stores/message/dynamodb').DynamoDBChatMessageHistory;\n",
    "\n",
    "var getMessageHistory = function(sessionId) {\n",
    "  return new DynamoDBChatMessageHistory({\n",
    "    tableName: \"CodeChat-Messages-LOCAL\",\n",
    "    partitionKey: \"id\",\n",
    "    sortKey: \"timestamp\",\n",
    "    sessionId: sessionId,\n",
    "    config: {\n",
    "      region: \"eu-west-1\",\n",
    "      endpoint: \"http://dynamodb:8000\",\n",
    "      credentials: {\n",
    "        accessKeyId: \"dummy\",\n",
    "        secretAccessKey: \"dummy\"\n",
    "      },\n",
    "    },\n",
    "  });\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable chain with memory\n",
    "\n",
    "- in order to have the platform manage memory for us we will need to create a `RunnableWithMessageHistory`\n",
    "- otherwise we will have to manually add messages of Human/AI to history and retrieve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var RunnableWithMessageHistory = require(\"@langchain/core/runnables\").RunnableWithMessageHistory;\n",
    "\n",
    "var chainWithHistory = new RunnableWithMessageHistory({\n",
    "    runnable,\n",
    "    getMessageHistory,\n",
    "    inputMessagesKey: \"input\",\n",
    "    outputMessagesKey: \"answer\",\n",
    "    historyMessagesKey: \"chat_history\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure input\n",
    "\n",
    "- let's start with the first step input asking about `LCEL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = 'What is LCEL ?';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM\n",
    "\n",
    "- let's invoke LLM as before, but providing as `configurable` parameter the sessionId that will retrieve from memory store the chat history for the respective user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainWithHistory.invoke(\n",
    "    { input },\n",
    "    { configurable: {\n",
    "        sessionId: \"testuser123\"\n",
    "        }\n",
    "    }\n",
    ").then((output) => {\n",
    "    console.log(output);\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainWithHistory.invoke(\n",
    "    { input: \"What is it?\" },\n",
    "    { configurable: {\n",
    "        sessionId: \"testuser123\"\n",
    "        }\n",
    "    }\n",
    ").then((output) => {\n",
    "    console.log(output);\n",
    "});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "20.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
