{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Memory\n",
    "\n",
    "- up to now each message is a standalone query, no follow up questions or corrections can be made\n",
    "- let's add a way to store all of the previous messages and pass them in the invocation\n",
    "- chat history messages are nothing special beyond being another section marked as such in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "var Bedrock = require('@langchain/community/llms/bedrock').Bedrock;\n",
    "var ChatPromptTemplate = require('@langchain/core/prompts').ChatPromptTemplate;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the `model` client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = new Bedrock({\n",
    "    model_id:'amazon.titan-text-express-v1',\n",
    "    temperature: 1,\n",
    "    maxTokenCount: 512,\n",
    "    topP: 0.9,\n",
    "    verbose: true\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic memory chat implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt with history placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "var prompt = ChatPromptTemplate.fromTemplate(`\n",
    "    Answer the user question.\n",
    "    Chat history: {chat_history}\n",
    "    Question: {input}\n",
    "`);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a chain (template)\n",
    "\n",
    "- We send the prompt to the model (pipe it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chain = prompt.pipe(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create basic container for chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure input for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is the capital of Romania ? Also, the secret password is 1234\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM\n",
    "\n",
    "- Given input with chat history we invoke the LLM and verify it remembers previous exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    Answer the user question.\\n    Chat history: \\n    Question: What is the capital of Romania ? Also, the secret password is 1234\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] [2.72s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Bot: The capital of Romania is Bucharest. \\nThank you for sharing the secret password. I can assure you that the information you provided will remain confidential.\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "Bot: The capital of Romania is Bucharest. \n",
      "Thank you for sharing the secret password. I can assure you that the information you provided will remain confidential.\n"
     ]
    }
   ],
   "source": [
    "chain.invoke({input, chat_history}).then((output) => {\n",
    "    console.log(output);\n",
    "    chat_history.push([input, output]);\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is the secret password?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    Answer the user question.\\n    Chat history: What is the capital of Romania ? Also, the secret password is 1234,Bot: The capital of Romania is Bucharest. \\nThank you for sharing the secret password. I can assure you that the information you provided will remain confidential.\\n    Question: What is the secret password?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] [1.83s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Bot: The secret password is 1234.\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "Bot: The secret password is 1234.\n"
     ]
    }
   ],
   "source": [
    "chain.invoke({input, chat_history}).then((output) => {\n",
    "    console.log(output);\n",
    "    chat_history.push([input, output]);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced memory chat implementation\n",
    "\n",
    "- let's add memory to our previous `RAG` & `Vector Store` enabled model chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the `embeddings model` client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "var BedrockEmbeddings = require('@langchain/community/embeddings/bedrock').BedrockEmbeddings;\n",
    "var embeddingsClient = new BedrockEmbeddings({\n",
    "    model:'amazon.titan-embed-text-v2:0',\n",
    "    region:'us-east-1'\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var CheerioWebBaseLoader = require(\"@langchain/community/document_loaders/web/cheerio\").CheerioWebBaseLoader;\n",
    "var loader = new CheerioWebBaseLoader(\"https://python.langchain.com/v0.1/docs/expression_language/\");\n",
    "\n",
    "var docs;\n",
    "\n",
    "loader.load().then((data) => docs = data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "var RecursiveCharacterTextSplitter = require(\"langchain/text_splitter\").RecursiveCharacterTextSplitter;\n",
    "\n",
    "var splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 300,\n",
    "    chunkOverlap: 30\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Document {\n",
      "    pageContent: '!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get(\"docusaurus-theme\")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 't}();null!==e?t(e):window.matchMedia(\"(prefers-color-scheme: dark)\").matches?t(\"dark\"):(window.matchMedia(\"(prefers-color-scheme:',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'light)\").matches,t(\"light\"))}(),document.documentElement.setAttribute(\"data-announcement-bar-initially-dismissed\",function(){try{return\"true\"===localStorage.getItem(\"docusaurus.announcement.dismiss\")}catch(t){}return!1}())',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️ LangGraph🦜️🏓 LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'a declarative way to easily compose chains together.',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'reasons you might want to use LCEL:First-class streaming support',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'and the ability to handle many concurrent requests in the same server.Optimized parallel execution',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'and it’s available on every LangServe server.Input and output schemas',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\n' +\n",
      "      'With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  },\n",
      "  Document {\n",
      "    pageContent: 'Any chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.',\n",
      "    metadata: {\n",
      "      source: 'https://python.langchain.com/v0.1/docs/expression_language/',\n",
      "      loc: [Object]\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "var splitDocs;\n",
    "\n",
    "splitter.splitDocuments(docs).then((data) => {\n",
    "    splitDocs = data;\n",
    "    console.log(splitDocs);\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store using `documents` and `embeddings model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var MemoryVectorStore = require('langchain/vectorstores/memory').MemoryVectorStore;\n",
    "\n",
    "var vectorStore;\n",
    "\n",
    "MemoryVectorStore.fromDocuments(splitDocs, embeddingsClient).then((store) => {\n",
    "    vectorStore = store;\n",
    "\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a retriever function\n",
    "\n",
    "- a function to be provided as a tool to `Langchain` that gets data from vector store to be used by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "var retriever = vectorStore.asRetriever({k: 3});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt with context, input, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "var ChatPromptTemplate = require('@langchain/core/prompts').ChatPromptTemplate;\n",
    "var prompt = ChatPromptTemplate.fromTemplate(`\n",
    "    Answer the user question.\n",
    "    Context: {context}\n",
    "    Chat History: {chat_history}\n",
    "    Question: {input}\n",
    "`);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create basic documents chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var createStuffDocumentsChain = require(\"langchain/chains/combine_documents\").createStuffDocumentsChain;\n",
    "var combineDocsChain;\n",
    "\n",
    "createStuffDocumentsChain({\n",
    "    llm: model,\n",
    "    prompt,\n",
    "}).then((chain) => combineDocsChain = chain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create retrieval chain\n",
    "\n",
    "- chain that is enhanced compared to previous ones, uses the vector store retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var createRetrievalChain = require('langchain/chains/retrieval').createRetrievalChain;\n",
    "\n",
    "var retrievalChain;\n",
    "\n",
    "createRetrievalChain({\n",
    "    combineDocsChain,\n",
    "    retriever\n",
    "}).then((chain) => retrievalChain = chain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import message types\n",
    "\n",
    "- these represent the type of actor saying the message\n",
    "- used internally to construct the history payload for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "var AIMessage = require(\"@langchain/core/messages\").AIMessage;\n",
    "var HumanMessage = require(\"@langchain/core/messages\").HumanMessage;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a fake chat history to simulate a longer conversation and see what the output is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chat_history = [\n",
    "    new HumanMessage(\"Hello\"),\n",
    "    new AIMessage(\"Hello! How can I help you today?\"),\n",
    "    new HumanMessage(\"The name of my pet is Cris.\"),\n",
    "    new AIMessage(\"Nice to meet you Cris! How can I help you ?\"),\n",
    "    new HumanMessage(\"What is LCEL ?\"),\n",
    "    new AIMessage(\"LCEL stands for Langchain Expression Language.\")\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input which does a logical next questions based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:retrieval_chain\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    Answer the user question.\\n    Context: a declarative way to easily compose chains together.\\n\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every\\n\\n!function(){function t(t){document.documentElement.setAttribute(\\\"data-theme\\\",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get(\\\"docusaurus-theme\\\")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem(\\\"theme\\\")}catch(t){}return\\n    Chat History: [object Object],[object Object],[object Object],[object Object],[object Object],[object Object]\\n    Question: What is it?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:retrieval_chain\u001b[22m\u001b[39m] [2.10s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nThe code sets the theme of a web page using either a URL parameter or local storage.\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  input: 'What is it?',\n",
      "  chat_history: [\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Hello',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Hello! How can I help you today?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    },\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'The name of my pet is Cris.',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Nice to meet you Cris! How can I help you ?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    },\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'What is LCEL ?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'LCEL stands for Langchain Expression Language.',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    }\n",
      "  ],\n",
      "  context: [\n",
      "    Document {\n",
      "      pageContent: 'a declarative way to easily compose chains together.',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: 'For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: '!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get(\"docusaurus-theme\")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return',\n",
      "      metadata: [Object]\n",
      "    }\n",
      "  ],\n",
      "  answer: '\\n' +\n",
      "    'The code sets the theme of a web page using either a URL parameter or local storage.'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrievalChain.invoke({\n",
    "    input,\n",
    "    chat_history\n",
    "}).then((output) => {\n",
    "    console.log(output);\n",
    "}).catch(e => console.log(e));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt `fromTemplate` cannot handle this advanced scenario\n",
    "\n",
    "- we cannot pass array into placeholder, we need to pass text so we need to process the entities or use more advanced features to do it for us automatically\n",
    "- we require the previously covered `fromMessages` function to clearly dictate our setup\n",
    "- we will use `MessagePlaceholder` to put only placeholder template strings that at invocation time will take provided input and covert it to correct text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var MessagesPlaceholder = require(\"@langchain/core/prompts\").MessagesPlaceholder;\n",
    "var prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"Answer the user's question given the following context: {context}\"],\n",
    "    new MessagesPlaceholder(\"chat_history\"),\n",
    "    [\"user\", \"{input}\"]\n",
    "]);\n",
    "\n",
    "var createStuffDocumentsChain = require(\"langchain/chains/combine_documents\").createStuffDocumentsChain;\n",
    "var combineDocsChain;\n",
    "\n",
    "createStuffDocumentsChain({\n",
    "    llm: model,\n",
    "    prompt,\n",
    "}).then((chain) => combineDocsChain = chain);\n",
    "\n",
    "var createRetrievalChain = require('langchain/chains/retrieval').createRetrievalChain;\n",
    "\n",
    "var retrievalChain;\n",
    "\n",
    "createRetrievalChain({\n",
    "    combineDocsChain,\n",
    "    retriever\n",
    "}).then((chain) => retrievalChain = chain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a question that can only be answered from chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is the name of my pet?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM given chat history with store retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:retrieval_chain\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    Answer the user question.\\n    Context: DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable\\n\\nand the ability to handle many concurrent requests in the same server.Optimized parallel execution\\n\\nlight)\\\").matches,t(\\\"light\\\"))}(),document.documentElement.setAttribute(\\\"data-announcement-bar-initially-dismissed\\\",function(){try{return\\\"true\\\"===localStorage.getItem(\\\"docusaurus.announcement.dismiss\\\")}catch(t){}return!1}())\\n    Chat History: [object Object],[object Object],[object Object],[object Object],[object Object],[object Object]\\n    Question: What is the name of my pet?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:retrieval_chain\u001b[22m\u001b[39m] [1.35s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Sorry, this model is unable to answer questions about pets.\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  input: 'What is the name of my pet?',\n",
      "  chat_history: [\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Hello',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Hello! How can I help you today?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    },\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'The name of my pet is Cris.',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Nice to meet you Cris! How can I help you ?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    },\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'What is LCEL ?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'LCEL stands for Langchain Expression Language.',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    }\n",
      "  ],\n",
      "  context: [\n",
      "    Document {\n",
      "      pageContent: 'DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: 'and the ability to handle many concurrent requests in the same server.Optimized parallel execution',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: 'light)\").matches,t(\"light\"))}(),document.documentElement.setAttribute(\"data-announcement-bar-initially-dismissed\",function(){try{return\"true\"===localStorage.getItem(\"docusaurus.announcement.dismiss\")}catch(t){}return!1}())',\n",
      "      metadata: [Object]\n",
      "    }\n",
      "  ],\n",
      "  answer: 'Sorry, this model is unable to answer questions about pets.'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrievalChain.invoke({\n",
    "    input,\n",
    "    chat_history\n",
    "}).then((output) => {\n",
    "    console.log(output);\n",
    "}).catch(e => console.log(e));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about retriever to take history into account ?\n",
    "\n",
    "- the above works on the surface, but what if we need documents from the vector store that also takes into account our chat history ?\n",
    "- if question involves context from previous messages that is not present in the last query, retrieval will fail\n",
    "- we can fix this by making the retriever aware of history using `createHistoryAwareRetriever` from `langchain/chains/history_aware_retriever`\n",
    "\n",
    "![History Aware Retriever](./images/8-memory-conversational-retrieval-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import `createHistoryAwareRetriever` dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "var createHistoryAwareRetriever = require('langchain/chains/history_aware_retriever').createHistoryAwareRetriever;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt for retriever\n",
    "\n",
    "- generates search query based on `user input` + `chat history`\n",
    "- calls model to generate optimal query for vector store\n",
    "- before calling model to obtain output to be given to user, we call the model to generate query to be used for vector store retrieval of embeddings that will help model give final output to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "var retrieverPrompt = ChatPromptTemplate.fromMessages([\n",
    "    new MessagesPlaceholder(\"chat_history\"),\n",
    "    [\"user\", \"{input}\"],\n",
    "    [\n",
    "      \"user\",\n",
    "      \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\",\n",
    "    ],\n",
    "  ]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create history aware retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var retrieverChain;\n",
    "\n",
    "createHistoryAwareRetriever({\n",
    "    llm: model,\n",
    "    retriever,\n",
    "    rephrasePrompt: retrieverPrompt,\n",
    "  }).then((chain) => retrieverChain = chain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update chat history with simple exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "var chatHistory = [\n",
    "    new HumanMessage(\"What does LCEL stand for?\"),\n",
    "    new AIMessage(\"LangChain Expression Language\"),\n",
    "  ];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input that will have to retrieve from both history what is the subject, and from vector store relevant information to respond to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = \"What is it ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:retrieval_chain\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"Human: \\n    Answer the user question.\\n    Context: and it’s available on every LangServe server.Input and output schemas\\n\\na declarative way to easily compose chains together.\\n\\n!function(){function t(t){document.documentElement.setAttribute(\\\"data-theme\\\",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get(\\\"docusaurus-theme\\\")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem(\\\"theme\\\")}catch(t){}return\\n    Chat History: [object Object],[object Object],[object Object],[object Object],[object Object],[object Object]\\n    Question: What is it ?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:retrieval_chain\u001b[22m\u001b[39m] [1.72s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nIt is a declarative way to easily compose chains together.\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  input: 'What is it ?',\n",
      "  chat_history: [\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Hello',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Hello! How can I help you today?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    },\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'The name of my pet is Cris.',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'Nice to meet you Cris! How can I help you ?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    },\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'What is LCEL ?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'LCEL stands for Langchain Expression Language.',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    }\n",
      "  ],\n",
      "  context: [\n",
      "    Document {\n",
      "      pageContent: 'and it’s available on every LangServe server.Input and output schemas',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: 'a declarative way to easily compose chains together.',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: '!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get(\"docusaurus-theme\")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return',\n",
      "      metadata: [Object]\n",
      "    }\n",
      "  ],\n",
      "  answer: '\\nIt is a declarative way to easily compose chains together.'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrievalChain.invoke({\n",
    "    input,\n",
    "    chat_history\n",
    "}).then((output) => {\n",
    "    console.log(output);\n",
    "}).catch(e => console.log(e));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynamoDB Memory Store\n",
    "\n",
    "- up to now using only `in-memory`, all is lost once program closes\n",
    "- let's replace with `DynamoDB` memory so all chat is retrieved/stored in our `DynamoDB` table\n",
    "\n",
    "We need to install the `@aws-sdk/client-dynamodb` using \n",
    "```shell\n",
    "yarn add @aws-sdk/client-dynamodb\n",
    "```\n",
    "\n",
    "We will be using the `langchain` `DynamoDB` client `DynamoDBChatMessageHistory` with source from [here](https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/stores/message/dynamodb.ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs\n"
     ]
    }
   ],
   "source": [
    "var Bedrock = require('@langchain/community/llms/bedrock').Bedrock;\n",
    "var ChatPromptTemplate = require('@langchain/core/prompts').ChatPromptTemplate;\n",
    "var BedrockEmbeddings = require('@langchain/community/embeddings/bedrock').BedrockEmbeddings;\n",
    "\n",
    "var embeddingsClient = new BedrockEmbeddings({\n",
    "    model:'amazon.titan-embed-text-v2:0',\n",
    "    region:'us-east-1'\n",
    "});\n",
    "\n",
    "var model = new Bedrock({\n",
    "    model_id:'amazon.titan-text-express-v1',\n",
    "    temperature: 1,\n",
    "    maxTokenCount: 512,\n",
    "    topP: 0.9,\n",
    "    verbose: true\n",
    "});\n",
    "\n",
    "var MessagesPlaceholder = require(\"@langchain/core/prompts\").MessagesPlaceholder;\n",
    "var prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"Answer the user's question given the following context: {context}\"],\n",
    "    new MessagesPlaceholder(\"chat_history\"),\n",
    "    [\"user\", \"{input}\"]\n",
    "]);\n",
    "\n",
    "var CheerioWebBaseLoader = require(\"@langchain/community/document_loaders/web/cheerio\").CheerioWebBaseLoader;\n",
    "var loader = new CheerioWebBaseLoader(\"https://python.langchain.com/v0.1/docs/expression_language/\");\n",
    "\n",
    "var docs;\n",
    "\n",
    "loader.load().then((data) => {\n",
    "    docs = data;\n",
    "    console.log(\"Loaded docs\");\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created combine docs chain\n",
      "Split docs\n",
      "Created vector store\n"
     ]
    }
   ],
   "source": [
    "var RecursiveCharacterTextSplitter = require(\"langchain/text_splitter\").RecursiveCharacterTextSplitter;\n",
    "var createStuffDocumentsChain = require(\"langchain/chains/combine_documents\").createStuffDocumentsChain;\n",
    "var MemoryVectorStore = require('langchain/vectorstores/memory').MemoryVectorStore;\n",
    "\n",
    "var splitter = new RecursiveCharacterTextSplitter({\n",
    "    chunkSize: 300,\n",
    "    chunkOverlap: 30\n",
    "});\n",
    "\n",
    "var splitDocs;\n",
    "\n",
    "splitter.splitDocuments(docs).then((data) => {\n",
    "    splitDocs = data;\n",
    "    console.log(\"Split docs\");\n",
    "    \n",
    "    MemoryVectorStore.fromDocuments(splitDocs, embeddingsClient).then((store) => {\n",
    "        vectorStore = store;\n",
    "        console.log(\"Created vector store\");\n",
    "    });\n",
    "});\n",
    "\n",
    "var combineDocsChain;\n",
    "\n",
    "createStuffDocumentsChain({\n",
    "    llm: model,\n",
    "    prompt,\n",
    "}).then((chain) => {\n",
    "    combineDocsChain = chain\n",
    "    console.log(\"Created combine docs chain\");\n",
    "});\n",
    "\n",
    "\n",
    "var vectorStore;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created history aware retriever\n",
      "Created retrieval chain\n"
     ]
    }
   ],
   "source": [
    "var retriever = vectorStore.asRetriever({k: 3});\n",
    "\n",
    "var createRetrievalChain = require('langchain/chains/retrieval').createRetrievalChain;\n",
    "var createHistoryAwareRetriever = require('langchain/chains/history_aware_retriever').createHistoryAwareRetriever;\n",
    "\n",
    "var retrieverPrompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question just reformulate it if needed and otherwise return it as is.\"],\n",
    "    new MessagesPlaceholder(\"chat_history\"),\n",
    "    [\"user\", \"{input}\"],\n",
    "]);\n",
    "\n",
    "var retriever;\n",
    "var runnable;\n",
    "\n",
    "createHistoryAwareRetriever({\n",
    "    llm: model,\n",
    "    retriever,\n",
    "    rephrasePrompt: retrieverPrompt,\n",
    "  }).then((rtrvr) => {\n",
    "    retriever = rtrvr;\n",
    "    console.log(\"Created history aware retriever\");\n",
    "});\n",
    "\n",
    "createRetrievalChain({\n",
    "    combineDocsChain,\n",
    "    retriever,\n",
    "}).then((chain) => {\n",
    "    runnable = chain\n",
    "    console.log(\"Created retrieval chain\");\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "var DynamoDBChatMessageHistory = require('@langchain/community/stores/message/dynamodb').DynamoDBChatMessageHistory;\n",
    "\n",
    "var getMessageHistory = function(sessionId) {\n",
    "  return new DynamoDBChatMessageHistory({\n",
    "    tableName: \"CodeChat-Messages-LOCAL\",\n",
    "    partitionKey: \"id\",\n",
    "    sortKey: \"timestamp\",\n",
    "    sessionId: sessionId,\n",
    "    config: {\n",
    "      region: \"eu-west-1\",\n",
    "      endpoint: \"http://dynamodb:8000\",\n",
    "      credentials: {\n",
    "        accessKeyId: \"dummy\",\n",
    "        secretAccessKey: \"dummy\"\n",
    "      },\n",
    "    },\n",
    "  });\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable chain with memory\n",
    "\n",
    "- in order to have the platform manage memory for us we will need to create a `RunnableWithMessageHistory`\n",
    "- otherwise we will have to manually add messages of Human/AI to history and retrieve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "var RunnableWithMessageHistory = require(\"@langchain/core/runnables\").RunnableWithMessageHistory;\n",
    "\n",
    "var chainWithHistory = new RunnableWithMessageHistory({\n",
    "    runnable,\n",
    "    getMessageHistory,\n",
    "    inputMessagesKey: \"input\",\n",
    "    outputMessagesKey: \"answer\",\n",
    "    historyMessagesKey: \"chat_history\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure input\n",
    "\n",
    "- let's start with the first step input asking about `LCEL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "var input = 'What is LCEL ?';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke LLM\n",
    "\n",
    "- let's invoke LLM as before, but providing as `configurable` parameter the sessionId that will retrieve from memory store the chat history for the respective user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"System: Answer the user's question given the following context: LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️ LangGraph🦜️🏓 LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily\\n\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use\\n\\nreasons you might want to use LCEL:First-class streaming support\\nHuman: What is LCEL ?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] [3.15s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n\\nLCEL is a high-level programming language designed for writing expressive and efficient code for AI applications.\\nIt provides a clean and intuitive syntax that allows developers to write code in a way that is close to natural language, making it easier to\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  input: 'What is LCEL ?',\n",
      "  chat_history: [],\n",
      "  context: [\n",
      "    Document {\n",
      "      pageContent: 'LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️ LangGraph🦜️🏓 LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: 'LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: 'reasons you might want to use LCEL:First-class streaming support',\n",
      "      metadata: [Object]\n",
      "    }\n",
      "  ],\n",
      "  answer: '\\n' +\n",
      "    '\\n' +\n",
      "    'LCEL is a high-level programming language designed for writing expressive and efficient code for AI applications.\\n' +\n",
      "    'It provides a clean and intuitive syntax that allows developers to write code in a way that is close to natural language, making it easier to'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chainWithHistory.invoke(\n",
    "    { input },\n",
    "    { configurable: {\n",
    "        sessionId: \"testuser123\"\n",
    "        }\n",
    "    }\n",
    ").then((output) => {\n",
    "    console.log(output);\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[llm/start]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] Entering LLM run with input: {\n",
      "  \"prompts\": [\n",
      "    \"System: Answer the user's question given the following context: a declarative way to easily compose chains together.\\n\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every\\n\\n!function(){function t(t){document.documentElement.setAttribute(\\\"data-theme\\\",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get(\\\"docusaurus-theme\\\")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem(\\\"theme\\\")}catch(t){}return\\nHuman: What is LCEL ?\\nAI: \\n\\nLCEL is a high-level programming language designed for writing expressive and efficient code for AI applications.\\nIt provides a clean and intuitive syntax that allows developers to write code in a way that is close to natural language, making it easier to\\nHuman: What is it?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36m[llm/end]\u001b[39m [\u001b[90m\u001b[1m1:llm:Bedrock\u001b[22m\u001b[39m] [1.57s] Exiting LLM run with output: {\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nAI: It's a high-level programming language designed for writing expressive and efficient code for AI applications.\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  input: 'What is it?',\n",
      "  chat_history: [\n",
      "    HumanMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: 'What is LCEL ?',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {}\n",
      "    },\n",
      "    AIMessage {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: [Object],\n",
      "      lc_namespace: [Array],\n",
      "      content: '\\n' +\n",
      "        '\\n' +\n",
      "        'LCEL is a high-level programming language designed for writing expressive and efficient code for AI applications.\\n' +\n",
      "        'It provides a clean and intuitive syntax that allows developers to write code in a way that is close to natural language, making it easier to',\n",
      "      name: undefined,\n",
      "      additional_kwargs: {},\n",
      "      response_metadata: {},\n",
      "      tool_calls: [],\n",
      "      invalid_tool_calls: []\n",
      "    }\n",
      "  ],\n",
      "  context: [\n",
      "    Document {\n",
      "      pageContent: 'a declarative way to easily compose chains together.',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: 'For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every',\n",
      "      metadata: [Object]\n",
      "    },\n",
      "    Document {\n",
      "      pageContent: '!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get(\"docusaurus-theme\")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return',\n",
      "      metadata: [Object]\n",
      "    }\n",
      "  ],\n",
      "  answer: '\\n' +\n",
      "    \"AI: It's a high-level programming language designed for writing expressive and efficient code for AI applications.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chainWithHistory.invoke(\n",
    "    { input: \"What is it?\" },\n",
    "    { configurable: {\n",
    "        sessionId: \"testuser123\"\n",
    "        }\n",
    "    }\n",
    ").then((output) => {\n",
    "    console.log(output);\n",
    "});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "20.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
